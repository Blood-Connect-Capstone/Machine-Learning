# -*- coding: utf-8 -*-
"""model_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13ymZKxB5njFNzJdVxjrZRiyiBFXKKukk

## Library and Install Depedencies
"""

# Install Depedencies
!pip install transformers==4.38.2 datasets==2.18.0 accelerate==0.27.2 bitsandbytes==0.42.0 peft==0.9.0 torch langchain==0.1.13 langchain_community==0.0.29
!pip install --upgrade numpy pandas transformers datasets torch

# For reuirement.txt
!pip freeze > requirements.txt

from google.colab import files
files.download('requirements.txt')

# Library
import numpy as np
import pandas as pd
import os

# Load Data dan Akses ke Drive
from google.colab import drive

# Data Processing and Model Building
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from datasets import Dataset, DatasetDict

"""## Load Data"""

# Mounting GDrive
drive.mount('/content/drive')

# Load dataset train dan validation
url_train = 'https://drive.google.com/uc?id=1sfxMl3R4bpJGiIPFibvdeqcg3AHyy9v-'
url_valid = 'https://drive.google.com/uc?id=1p6SVXXlEsZrg2QAgu34bdgsHHI0d6MOq'

data_train = pd.read_csv(url_train)
data_valid = pd.read_csv(url_valid)

# Menampilkan 5 data pada data_train
data_train.head()

# Menampilkan 5 data pada data_train
data_valid.head()

# Menggabungkan Data
#data = pd.concat([data_train, data_valid], ignore_index = True)
#data.to_csv("Medical.csv", index = False)
#data.head()

"""## Data Prepocessing"""

# Konversi Pandas DataFrame ke Hugging Face Dataset
hf_train = Dataset.from_pandas(data_train)
hf_valid = Dataset.from_pandas(data_valid)

# Cek struktur dataset HF yang baru
print("\nHugging Face Train Dataset Columns:", hf_train.column_names)
print("Hugging Face Valid Dataset Columns:", hf_valid.column_names)

# Inisialisasi Tokenizer
model_name = "EleutherAI/pythia-160m"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Set pad_token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    print(f"Tokenizer pad_token set to: {tokenizer.pad_token}")

# Fungsi agar data bisa digunakan oleh model
def preprocess_function_causal_lm(examples):
    # Menggabungkan QnA ke dalam format prompt dan menambahkan tokenizer.eos_token (End-Of-Sequence) di akhir answer
    texts = [
        f"### Question:\n{q.strip()}\n\n### Answer:\n{a.strip()}{tokenizer.eos_token}"
        for q, a in zip(examples['short_question'], examples['short_answer'])
    ]
    # Melakukan tokenisasi
    model_inputs = tokenizer(
        texts,
        truncation=True,
        padding="max_length",
        max_length=512,
    )
    model_inputs["labels"] = model_inputs["input_ids"].copy()

    return model_inputs

# Melakukan preprocessing ke dataset train
print("\nApplying preprocessing to train dataset...")
tokenized_train_dataset = hf_train.map(
    preprocess_function_causal_lm,
    batched=True,
    remove_columns=hf_train.column_names
)

# Melakukan preprocessing ke dataset valid
print("\nApplying preprocessing to validation dataset...")
tokenized_valid_dataset = hf_valid.map(
    preprocess_function_causal_lm,
    batched=True,
    remove_columns=hf_valid.column_names
)

# Gabungkan dataset train dan valid ke dalam DatasetDict (Format Hugging Face Trainer)
tokenized_dataset = DatasetDict({
    "train": tokenized_train_dataset,
    "validation": tokenized_valid_dataset,
})

print("\nFinal Tokenized Dataset Structure:")
print(tokenized_dataset)
print("Example of tokenized train data entry:")
print(tokenized_dataset["train"][0])

"""## Base Directory"""

# Menyimpan semua proses build model ke drive
base_dir = "/content/drive/MyDrive/Capstone/best_model"

"""## Build Model"""

# Muat model untuk Causal Language Modeling
model = AutoModelForCausalLM.from_pretrained(model_name)

# Konfigurasi argumen pelatihan
training_args = TrainingArguments(
    output_dir=f"{base_dir}/results",
    num_train_epochs=10,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    warmup_steps=100,
    weight_decay=0.01,
    logging_dir=f"{base_dir}/logs",
    logging_steps=10,
    save_strategy="epoch",
    load_best_model_at_end=False,
    evaluation_strategy="epoch",
    report_to="tensorboard",
)

# Data Collator untuk menyiapkan batch data
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

"""## Training and Testing"""

# Inisialisasi Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"], # Split 'validation'
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# Mulai pelatihan
print("\nStarting model training...")
trainer.train()
print("\nTraining finished!")

"""## Evaluation"""

# Melihat metric akurasi model
evaluation_results = trainer.evaluate()
print("\nHasil Evaluasi Akhir:")
print(evaluation_results)

"""## Save Model"""

# Menyimpan model
output_model_dir = "./fine-tuned-chatbot-model"
trainer.save_model(output_model_dir)
tokenizer.save_pretrained(output_model_dir)
print(f"\nFine-tuned model saved to {output_model_dir}")

# Menyimpan hasil di drive
best_model = f"{output_model_dir}/final_model"

# Menyimpan model dan tokenizer ke Google Drive
print(f"\nMenyimpan model ke {base_dir}...")
trainer.save_model(best_model)
tokenizer.save_pretrained(best_model)
print(f"\nModel fine-tuned berhasil disimpan ke: {base_dir}")
