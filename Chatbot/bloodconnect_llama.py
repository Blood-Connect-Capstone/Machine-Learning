# -*- coding: utf-8 -*-
"""bloodconnect-llama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BLCiHbJC8rPijHkr17a2PtXANs8XvuV3
"""

# Install Depedencies
!pip install -q transformers sentence-transformers faiss-cpu

# Library
import pandas as pd
import numpy as np

from sentence_transformers import SentenceTransformer
import faiss

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# load dataset
df = pd.read_csv('/content/bloodconnect_faq_dataset.csv')
df.head()

# Memisahkan question dan answer
questions = df['question'].tolist()
answers = df['answer'].tolist()

# Embedding FAQ dengan MiniLM
retriever = SentenceTransformer("all-MiniLM-L6-v2")
faq_embeddings = retriever.encode(questions, convert_to_numpy=True)

# Buat FAISS index
index = faiss.IndexFlatL2(faq_embeddings.shape[1])
index.add(faq_embeddings)

# Load model

model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")

chatbot = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto",
    pad_token_id=tokenizer.eos_token_id
)

def rag_tinyllama_chat(user_question, top_k=1):

    # Mengambil pertanyaan relevan dari FAQ (retrieval)
    query_vec = retriever.encode([user_question], convert_to_numpy=True)
    D, I = index.search(np.array(query_vec), top_k)

    # Menyusun konteks dari pertanyaan serupa
    context = "\n".join([f"Q: {questions[i]}\nA: {answers[i]}" for i in I[0]])

    # Menyusun prompt
    prompt = f"""You are Assistant BloodConnect.

    Jawablah pertanyaan pengguna berdasarkan informasi di bawah ini.

    {context}

    Pertanyaan: {user_question}
    Jawaban:"""

    # Generate jawaban
    output = chatbot(prompt, max_new_tokens=150, temperature=0.7)
    return output[0]['generated_text']

# Testing
response = rag_tinyllama_chat("Apa manfaat donor darah?")
print(response)

# Testing
response = rag_tinyllama_chat("Apa itu bloodconnect?")
print(response)

# Menyimpan model
model.save_pretrained("tinyllama_model_bc")
tokenizer.save_pretrained("tinyllama_model_bc")

# Save model ke drive
from google.colab import drive
drive.mount('/content/drive')

save_path = "/content/drive/MyDrive/models/tinyllama_model_bc"
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

"""# Hubungin ke HuggingFace"""

from huggingface_hub import login
login(token="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")

from huggingface_hub import HfApi

api = HfApi()

# Buat repo model
api.create_repo(repo_id="geraldalivia/tinyllama-bloodconnect", repo_type="model", private=False)

# Upload folder model dari Drive
from huggingface_hub import upload_folder

upload_folder(
    repo_id="geraldalivia/tinyllama-bloodconnect",
    folder_path="/content/drive/MyDrive/models/tinyllama_model_bc",
    path_in_repo=".",
    repo_type="model"
)

from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "geraldalivia/tinyllama-bloodconnect"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

from transformers import AutoTokenizer, AutoModelForCausalLM

# Model dan tokenizer dari TinyLLaMA
model = AutoModelForCausalLM.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")

# Simpan dan push langsung ke repo kamu
model.save_pretrained("bloodconnect_llama", push_to_hub=True, repo_id="geraldalivia/llama-bloodconnect", safe_serialization=False)
tokenizer.save_pretrained("cloodconnect_llama", push_to_hub=True, repo_id="geraldalivia/llama-bloodconnect")